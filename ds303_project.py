# -*- coding: utf-8 -*-
"""DS303-Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MtIEcDxNqE6mA7Lmw9qgnKYfdV9WEZ-M
"""

from google.colab import drive
drive.mount('/content/drive')

"""<center><h1><b> DS303 Course Project </b></h1> </center>
<center> Spring Semester 2021, Under Prof. Biplab Banarjee </center>

Group members:


*   Sanket Ghyar - 190110023 - Metallurgical Engineering and Materials Science, 2nd Year
*   Arif Ahmad - 190110010 - Electrical Engineering, BTech 2nd Year
*   Naresh Balamurgan - 190110051 - Metallurgical Engineering and Materials Science, 2nd Year
*   Yash Shah - 190020136 - Chemical Engineering, BTech 2nd Year

<h2>Importing required Libraries </h2>
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import cv2

import tensorflow as tf
import tensorflow_datasets as tfds

from keras.datasets import mnist
from keras.models import Sequential, load_model
from keras.layers.core import Dense, Dropout, Activation
from tensorflow.keras.layers import Flatten,Conv2D, MaxPooling2D
from keras.utils import np_utils



"""<h3><b>We have used multiple machine learning methods like Neural Networks, Convolutional Neural Networks and SVM to train a machine learning model on the MNIST dataset; capable of classifying digits.</b></h3>

<h1><b>1. Classification using Neural Network</b></h1>

<h4>Loading Data from keras library and analyzing it</h4>
"""

(X_train, y_train), (X_test, y_test) = mnist.load_data()

print(f"Size of training data set is: {X_train.shape}")
print(f"Size of test data set is: {X_test.shape}")
print(f"Size of training labels' set is: {y_train.shape}")
print(f"Size of test labels' set is: {y_test.shape}")

print(y_train)
print('As can be seen above the labels of the data set are not one-hot encoded and carry the labels as the numbers they represnt.')

#Plotting the first few digits in dataset as image
fig = plt.figure()
for i in range(16):
  plt.subplot(4,4,i+1)
  plt.tight_layout()
  plt.imshow(X_train[i], cmap='gray', interpolation='none')
  plt.title(f"Digit: {y_train[i]}")
  plt.xticks([])
  plt.yticks([])

"""<h3><b>Pixel Distribution and Data Normalization</b></h3>"""

plt.hist(X_train[0])
plt.title("Pixel Value Distribution")

"""As we can see in the above plot; the pixel values range between 0 and 255 in the 28x28 image. We can normalize the pixel values to lie between 0 and 1, to help spped up training.

<h3><b> Normalizing data </b> </h3>

We will also reshape the 28x28 image input to a 784 sized input feature vector.
"""

#reshaping the data feature size from 28x28 to single vector of size 784
X_train = X_train.reshape(60000,784)
X_test = X_test.reshape(10000,784)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

#Normalizing Data
X_train /= 255
X_test /= 255

#Printing final input shape ready for training
print(f'Train Matrix Shape: {X_train.shape}')
print(f'Test Matrix Shape: {X_test.shape}')

"""<h3><b>One hot encoding the labels</b></h3>

As can be seen below the labels are from the set {0,1,2,3,..8,9} in both the training and test data sets, however it would be desirable to have one hot encoded labels for ease of interpretation of the output produced after an input runs through the neural network as well as for training the model.
"""

#Priniting unique values in y_train and y_test and their counts
print(np.unique(y_train, return_counts=True))
print(np.unique(y_test, return_counts=True))

#one-hot encoding using numpy
n_classes = 10
print(f'Shape before one hot encoding: {y_train.shape}')
Y_train = np_utils.to_categorical(y_train, n_classes)
Y_test = np_utils.to_categorical(y_test, n_classes)
print(f'Shape after one hot encoding: {Y_train.shape}')

"""<br>

<h1><b>Building the Neural Network</b></h1>

We will be building a neural network with two hidden 512-node layers. And for multi-class classification we add another densely connected layer for the 10 output classes. The input vector is of size 784. We will be using the softmax activation function in the last layer for connections to the 10 class nodes. Softmax activation function is used as it is common to use softmax activation for training in multi-class classification.

We will be using the ReLU activation function in the hidden layers.
"""

#building a linear stack of layers using Sequential Model in Keras

model = Sequential()

#Adding first Hidden Layer
model.add(Dense(512, input_shape=(784,)))
model.add(Activation('relu'))
model.add(Dropout(0.2))

#Adding Second Hidden Layer
model.add(Dense(512))
model.add(Activation('relu'))
model.add(Dropout(0.2))

#Softmax Classification Layer
model.add(Dense(10))
model.add(Activation('softmax'))

"""<h3><b> Summary Of model </h3>"""

#Summary of model
model.summary()

"""<h2><b>Training the model</b></h2>

Categorical Cross Entropy will be used as a loss function for optimizing model parameters. And we are using Stochastic Gradient Descent (SGD) for optimization.

Compiling
"""

#Compiling the Sequential Model
model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='SGD')

"""Training"""

#training the model and saving the metrics in saved_model

saved_model = model.fit(X_train, Y_train, batch_size=128, epochs=20, verbose=2, validation_data=(X_test, Y_test))

"""<h3><b>Plotting the metrics</b></h3>"""

fig = plt.figure()

plt.subplot(2,1,1)
plt.plot(saved_model.history['accuracy'])
plt.plot(saved_model.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='lower right')

plt.subplot(2,1,2)
plt.plot(saved_model.history['loss'])
plt.plot(saved_model.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')

plt.tight_layout()

"""<h1> <b> Evaluating Model Perormance </b> </h1>"""

loss_and_metrics = model.evaluate(X_test, Y_test, verbose=2)

print(f'Test loss: {loss_and_metrics[0]}')
print(f'Test Accuracy: {loss_and_metrics[1]}')

"""Below we check the exact numbers of correctly classified and missclafied samples from the test data set. We also see in picture which samples are misclassified and find that the miss classified samples do have some inherent ambiguity in which number they represent due to unclear strokes."""

# load the model and create predictions on the test set
predicted_classes = model.predict_classes(X_test)

# see which we predicted correctly and which not
correct_indices = np.nonzero(predicted_classes == y_test)[0]
incorrect_indices = np.nonzero(predicted_classes != y_test)[0]
print()
print(len(correct_indices)," classified correctly")
print(len(incorrect_indices)," classified incorrectly")

# adapt figure size to accomodate 18 subplots
plt.rcParams['figure.figsize'] = (7,14)

figure_evaluation = plt.figure()

# plot 9 correct predictions
for i, correct in enumerate(correct_indices[:9]):
    plt.subplot(6,3,i+1)
    plt.imshow(X_test[correct].reshape(28,28), cmap='gray', interpolation='none')
    plt.title(
      "Predicted: {}, Truth: {}".format(predicted_classes[correct],
                                        y_test[correct]))
    plt.xticks([])
    plt.yticks([])

# plot 9 incorrect predictions
for i, incorrect in enumerate(incorrect_indices[:9]):
    plt.subplot(6,3,i+10)
    plt.imshow(X_test[incorrect].reshape(28,28), cmap='gray', interpolation='none')
    plt.title(
      "Predicted {}, Truth: {}".format(predicted_classes[incorrect], 
                                       y_test[incorrect]))
    plt.xticks([])
    plt.yticks([])

"""<br><br><br>
<h1><b>2. Classification using Convolutional Neural Network</b></h1>

Now, we try to do the same classification using a more advanced method - Convolutional Neural Networks, and try to guage any advantages or disadvantages in the same.

As we have already pre-processed the data; we just need to create the model using CNN now, train it, test it, and see its performance and accuracy
"""

(X_train, y_train), (X_test, y_test) = mnist.load_data()

"""### Normalizing the data"""

X_train = tf.keras.utils.normalize(X_train, axis = 1)
X_test = tf.keras.utils.normalize(X_test, axis = 1)

"""### Resizing image for Convolutional layer"""

img_size = 28
X_trainr = np.array(X_train).reshape(-1, img_size, img_size, 1)
X_testr = np.array(X_test).reshape(-1, img_size, img_size, 1)
print('Training images dimensions', X_trainr.shape)
print('Testing images dimensions', X_testr.shape)
print(len(X_trainr))

"""<h1><b>Building the Convolutional Neural Network</b></h1>

In building this Convolutional Neural Network, we have used three CNN layers; each consisting of 64 layers and a 3x3 kernel densely connected and used a softmax layer at the end for multiclass classification.
"""

model = Sequential()

"""#### Convolutional Layer 1"""

model.add(Conv2D(64,(3,3), input_shape = X_trainr.shape[1:]))
model.add(Activation("relu"))
model.add(Dropout(0.2))
model.add(MaxPooling2D(pool_size=(2,2)))

"""#### Convolutional Layer 2"""

model.add(Conv2D(64,(3,3), input_shape = X_trainr.shape[1:]))
model.add(Activation("relu"))
model.add(Dropout(0.2))
model.add(MaxPooling2D(pool_size=(2,2)))

"""#### Convolutional Layer 3"""

model.add(Conv2D(64,(3,3), input_shape = X_trainr.shape[1:]))
model.add(Activation("relu"))
model.add(Dropout(0.2))
model.add(MaxPooling2D(pool_size=(2,2)))

"""#### Fully Connected Layer 1"""

model.add(Flatten())
model.add(Dense(64))
model.add(Activation('relu'))

"""#### Fully Connected Layer 2"""

model.add(Dense(32))
model.add(Activation('relu'))

"""#### Fully Connected Layer 3"""

model.add(Dense(16))
model.add(Activation('relu'))

"""#### Last Fully Connected Layer """

model.add(Dense(10))
model.add(Activation('softmax'))

"""<h3><b> Summary of Model </b> </h3>"""

model.summary()

"""<h2><b>Training the model</b></h2>

Compiling
"""

model.compile(loss = 'sparse_categorical_crossentropy', optimizer ='adam', metrics=['accuracy'])

"""Training"""

history = model.fit(X_trainr, y_train, epochs = 5, validation_data= (X_trainr, y_train))

"""<h3><b>Plotting the metrics</b></h3>"""

fig = plt.figure()

plt.subplot(2,1,1)
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train','test'], loc='lower right')

plt.subplot(2,1,2)
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper right')

plt.tight_layout()

"""<h1> <b> Evaluating Model Perormance </b> </h1>"""

loss_and_metrics = model.evaluate(X_testr, y_test, verbose=2)

print(f'Test loss: {loss_and_metrics[0]}')
print(f'Test Accuracy: {loss_and_metrics[1]}')

"""Below we check the exact numbers of correctly classified and missclafied samples from the test data set. We also see in picture which samples are misclassified and find that the miss classified samples do have some inherent ambiguity in which number they represent due to unclear strokes."""

# load the model and create predictions on the test set
predicted_classes = model.predict_classes(X_testr)

# see which we predicted correctly and which not
correct_indices = np.nonzero(predicted_classes == y_test)[0]
incorrect_indices = np.nonzero(predicted_classes != y_test)[0]
print()
print(len(correct_indices)," classified correctly")
print(len(incorrect_indices)," classified incorrectly")

# adapt figure size to accomodate 18 subplots
plt.rcParams['figure.figsize'] = (7,14)

figure_evaluation = plt.figure()

# plot 9 correct predictions
for i, correct in enumerate(correct_indices[:9]):
    plt.subplot(6,3,i+1)
    plt.imshow(X_testr[correct].reshape(28,28), cmap='gray', interpolation='none')
    plt.title(
      "Predicted: {}, Truth: {}".format(predicted_classes[correct],
                                        y_test[correct]))
    plt.xticks([])
    plt.yticks([])

# plot 9 incorrect predictions
for i, incorrect in enumerate(incorrect_indices[:9]):
    plt.subplot(6,3,i+10)
    plt.imshow(X_testr[incorrect].reshape(28,28), cmap='gray', interpolation='none')
    plt.title(
      "Predicted {}, Truth: {}".format(predicted_classes[incorrect], 
                                       y_test[incorrect]))
    plt.xticks([])
    plt.yticks([])

"""<br><br><br>
<h1><b>3. Classification using  Support Vector Machines</b></h1>
"""

(X_train, y_train), (X_test, y_test) = mnist.load_data()

#reshaping the data feature size from 28x28 to single vector of size 784
X_train = X_train.reshape(60000,784)
X_test = X_test.reshape(10000,784)
X_train = X_train.astype('float32')
X_test = X_test.astype('float32')

## Normalization

X_train = X_train/255.0
X_test= X_test/255.0

print("Train_Data:", X_train.shape)
print("Test_Data:",X_test.shape)

from sklearn import preprocessing
# Create the Scaler object
scaler = preprocessing.StandardScaler()
# Fit your data on the scaler object
scaled_X_train = scaler.fit_transform(X_train)
scaled_X_test = scaler.fit_transform(X_test)

"""<h3><b>SVM LINEAR KERNEL MODEL BUILDING</b></h3>"""

# linear model
from sklearn.svm import SVC
model_linear = SVC(kernel='linear')
model_linear.fit(scaled_X_train, y_train)

# predict
y_pred = model_linear.predict(scaled_X_test)

# confusion matrix and accuracy

from sklearn import metrics
from sklearn.metrics import confusion_matrix
# accuracy
print("accuracy:", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), "\n")

# cm
print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))

"""<h3><b>SVM RBF KERNEL MODEL BUILDING</b></h3>"""

# non-linear model
# using rbf kernel, C=1, default value of gamma

# model
non_linear_model = SVC(kernel='rbf')

# fit
non_linear_model.fit(scaled_X_train, y_train)

# predict
y_pred = non_linear_model.predict(scaled_X_test)

# confusion matrix and accuracy

# accuracy
print("accuracy:", metrics.accuracy_score(y_true=y_test, y_pred=y_pred), "\n")

# cm
print(metrics.confusion_matrix(y_true=y_test, y_pred=y_pred))